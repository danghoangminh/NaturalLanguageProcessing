from underthesea import word_tokenize

list_word = []

with open('word.txt', 'r', encoding='utf-8') as f:
    data = f.read()